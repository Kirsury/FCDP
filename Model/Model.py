import os
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
import json
import tqdm
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from transformers import RobertaModel, RobertaTokenizer
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score
from collections import Counter
from datetime import datetime


"""
ç¼ºé™·é¢„æµ‹æ¨¡å‹è®¾è®¡ã€å®ç°å’Œæµ‹è¯•è¯„ä¼°
"""

# 1. é…ç½®è®¾å¤‡
try:
    import torch_npu
    npu_available = torch.npu.is_available()
except:
    npu_available = False

device = torch.device("npu:6" if npu_available else "cuda:0" if torch.cuda.is_available() else "cpu")
print(f"âœ… ä½¿ç”¨è®¾å¤‡{device}")

# 2. æ—¥å¿—å’Œæ¨¡å‹ä¿å­˜è·¯å¾„è®¾ç½®
"""
è®°å½•è®­ç»ƒã€æµ‹è¯•è¯„ä¼°è¿‡ç¨‹ä¸­çš„å…³é”®ä¿¡æ¯ï¼Œä»¥ä¾¿äºå›é¡¾å’Œåˆ†æã€‚
æ¯æ¡æ—¥å¿—ä¿¡æ¯åº”åŒ…å«æ—¶é—´ä¿¡æ¯ï¼ˆå¹´æœˆæ—¥æ—¶åˆ†ç§’ï¼‰ã€‚
"""
def current_time():
    return datetime.now().strftime("%Y%m%d_%H%M%S")

import logging

log_path = f"log_{datetime.now().isoformat()}.txt"
logging.basicConfig(filename=log_path, level=logging.INFO, format='[%(asctime)s] %(message)s')

def log(msg):
    print(msg)
    logging.info(msg)




# 3. è¶…å‚æ•°è®¾ç½®
num_epoch = 5
batch_size = 16
max_len = 512
learning_rate = 2e-5
freeze_layers = 6
model_save_dir = "./saved_models"
os.makedirs(model_save_dir, exist_ok=True)


# 4. è®­ç»ƒé›†å’Œæµ‹è¯•é›†è·¯å¾„
train_path = "./Dataset/cleaned/249.json"
test_path = "./Dataset/cleaned/250.json"

# 5. åŠ è½½æ•°æ®
def load_jsonlines(path):
    with open(path, 'r', encoding='utf-8') as f:
        data = json.load(f)
        return [{"code": item["code"], "label": item["label"]} for item in data]

print(f"âœ… åŠ è½½è®­ç»ƒæ•°æ®é›†...")
train_data = load_jsonlines(train_path)
print(f"âœ… åŠ è½½æµ‹è¯•æ•°æ®é›†...")
test_data = load_jsonlines(test_path)

# 6. åŠ è½½tokenizer
tokenizer = RobertaTokenizer.from_pretrained("microsoft/codebert-base")



# 7. è‡ªå®šä¹‰ Dataset
class CodeDataset(Dataset):
    def __init__(self, data, tokenizer, max_len=512):
        self.data = data
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        code = self.data[idx]['code']
        label = self.data[idx]['label']
        encoding = self.tokenizer(code, padding='max_length', truncation=True, max_length=self.max_len, return_tensors='pt')
        return {
            'input_ids': encoding['input_ids'].squeeze(0),  # [seq_len]
            'attention_mask': encoding['attention_mask'].squeeze(0),
            'labels': torch.tensor(label, dtype=torch.long)
        }

print(f"âœ… åˆ›å»ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†...")
train_dataset = CodeDataset(train_data, tokenizer)
test_dataset = CodeDataset(test_data, tokenizer)
train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)
test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)

# 8. è§£å†³ç±»ä¸å¹³è¡¡
label_counts = Counter([item['label'] for item in train_data])
total = sum(label_counts.values())
weights = [total / label_counts[i] for i in range(len(label_counts))]  # inverse freq
class_weights = torch.tensor(weights, dtype=torch.float).to(device)
criterion = torch.nn.CrossEntropyLoss(weight=class_weights)
print(f"âœ… ç±»æƒé‡: {class_weights}")

# 9. å®šä¹‰æ¨¡å‹
class DefectPredictionModel(nn.Module):
    def __init__(self, pretrained_model_name='microsoft/codebert-base', freeze_layers=6):
        super(DefectPredictionModel, self).__init__()
        self.encoder = RobertaModel.from_pretrained(pretrained_model_name)

        # æ­£ç¡®å†»ç»“åº•éƒ¨6å±‚
        for name, param in self.encoder.named_parameters():
            if name.startswith("encoder.layer."):
                layer_num = int(name.split(".")[2])
                if layer_num < freeze_layers:
                    param.requires_grad = False

        # æ‰“å°è¢«å†»ç»“çš„å‚æ•°å
        for name, param in model.encoder.named_parameters():
            if not param.requires_grad:
                print("Frozen:", name)

        self.hidden_size = self.encoder.config.hidden_size
        self.classifier = nn.Sequential(
            nn.Linear(self.hidden_size, self.hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(self.hidden_size // 2, 2)  # äºŒåˆ†ç±»ï¼šç¼ºé™· / éç¼ºé™·
        )

    def forward(self, input_ids, attention_mask):
        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)
        cls_rep = outputs.last_hidden_state[:, 0, :]  # [CLS] token è¡¨å¾
        logits = self.classifier(cls_rep)
        return logits

print(f"âœ… åˆ›å»ºæ¨¡å‹...")
model = DefectPredictionModel(freeze_layers=freeze_layers).to(device)
optimizer = torch.optim.AdamW(lambda p: p.requires_grad, filter(model.parameters()), lr=2e-5)


# 10. è¯„ä¼°æŒ‡æ ‡
def compute_metrics(logits, targets):
    preds = torch.argmax(logits, dim=1).cpu().numpy()
    targets = targets.cpu().numpy()

    acc = accuracy_score(targets, preds)
    precision, recall, f1, _ = precision_recall_fscore_support(targets, preds, average='binary')
    try:
        prob = F.softmax(torch.tensor(logits), dim=1)[:, 1].detach().cpu().numpy()
        auc = roc_auc_score(targets, prob)
    except:
        auc = 0.0  # AUC æ— æ³•è®¡ç®—ï¼ˆå¦‚å…¨1æˆ–å…¨0ï¼‰

    return {
        'accuracy': acc,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'auc': auc
    }

# 11. æ—¥å¿—init

print(f"âœ… åˆå§‹åŒ–æ—¥å¿—...")



# 12. ğŸ” è®­ç»ƒä¸»å¾ªç¯
print(f"âœ… å¼€å§‹è®­ç»ƒ...")
for epoch in range(5):
    """
    æ¯è½®çš„modeléƒ½è¦ä¿å­˜ä¸‹æ¥ï¼Œæ¨¡å‹åè¦åŒ…å«å½“å‰çš„å¹´æœˆæ—¥æ—¶åˆ†ç§’ä¿¡æ¯å’Œepoch idä¿¡æ¯ä»¥ä½¿åç§°å”¯ä¸€åŒ–å’Œä¾¿äºç®¡ç†åŒ–
    """
    model.train()
    total_loss, num_steps = 0, 0
    for batch in tqdm.tqdm(train_dataloader, "Training"):
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        logits = model(input_ids, attention_mask)
        loss = criterion(logits, labels)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss
        num_steps += 1
    avg_loss = total_loss / num_steps

    print(f"Epoch {epoch + 1}/{num_epoch} | train loss: {avg_loss.item():.4f}")
    model_path = os.path.join(model_save_dir, f"model_{current_time()}_epoch{epoch + 1}.pt")
    torch.save(model.state_dict(), model_path)
    log(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ° {model_path}")

    # write train log

# 13. ğŸ§ª åœ¨ test set ä¸Šè¯„ä¼°å¹³å‡æŒ‡æ ‡
model.eval()
all_logits = []
all_labels = []

with torch.no_grad():
    for batch in test_dataloader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        logits = model(input_ids, attention_mask)
        all_logits.append(logits)
        all_labels.append(labels)

# èšåˆå¹¶è¯„ä¼°
all_logits = torch.cat(all_logits, dim=0)
all_labels = torch.cat(all_labels, dim=0)
test_metrics = compute_metrics(all_logits, all_labels)

"""
ä¿å­˜logitså’Œlabelsï¼Œä»¥åŠå¯¹åº”çš„codeï¼Œæ–‡ä»¶ååº”åŒ…å«ç›¸åº”çš„æ¨¡å‹åå’Œepoch numä¿¡æ¯
ä»¥ä¾¿äºåç»­åˆ†æå…·ä½“çš„point
"""

print("Test Metrics:")
for k, v in test_metrics.items():
    print(f"{k}: {v:.4f}")

output_path = f"outputs/predict_{current_time()}_epoch{epoch+1}.jsonl"
os.makedirs("outputs", exist_ok=True)

with open(output_path, 'w', encoding='utf-8') as f:
    for logit, label, item in zip(all_logits, all_labels, test_data):
        prob = F.softmax(logit, dim=0).tolist()
        entry = {
            "code": item["code"],
            "label": int(label),
            "prob_0": prob[0],
            "prob_1": prob[1]
        }
        f.write(json.dumps(entry, ensure_ascii=False) + "\n")

log(f"âœ… ä¿å­˜æµ‹è¯•è¾“å‡ºè‡³ {output_path}")
